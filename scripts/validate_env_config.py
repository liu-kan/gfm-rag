#!/usr/bin/env python3
"""
GFM-RAG ç¯å¢ƒå˜é‡é…ç½®éªŒè¯è„šæœ¬

è¯¥è„šæœ¬ç”¨äºéªŒè¯å’Œæ£€æŸ¥GFM-RAGç³»ç»Ÿçš„ç¯å¢ƒå˜é‡é…ç½®çŠ¶æ€ï¼Œ
ç¡®ä¿Hydraé…ç½®æ–‡ä»¶èƒ½å¤Ÿæ­£ç¡®è¯»å–å’Œä½¿ç”¨ç¯å¢ƒå˜é‡ã€‚

ç”¨æ³•:
    python scripts/validate_env_config.py
    
    # æˆ–è€…
    python -m scripts.validate_env_config
"""

import os
import sys
import logging
from typing import Dict, Any
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from gfmrag.config_manager import get_config_manager
    from gfmrag.kg_construction.langchain_util import init_langchain_model
    from hydra import compose, initialize_config_dir
    from omegaconf import DictConfig, OmegaConf
except ImportError as e:
    print(f"é”™è¯¯ï¼šæ— æ³•å¯¼å…¥å¿…è¦çš„æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿æ‚¨åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬ï¼Œå¹¶ä¸”å·²å®‰è£…æ‰€æœ‰ä¾èµ–é¡¹ã€‚")
    sys.exit(1)

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def check_environment_variables() -> Dict[str, Any]:
    """æ£€æŸ¥GFMRAGç›¸å…³ç¯å¢ƒå˜é‡çš„è®¾ç½®çŠ¶æ€"""
    print("=" * 60)
    print("ğŸ”§ GFM-RAG ç¯å¢ƒå˜é‡é…ç½®æ£€æŸ¥")
    print("=" * 60)
    
    env_vars = {
        'GFMRAG_CHAT_PROVIDER': 'ChatæœåŠ¡æä¾›å•†',
        'GFMRAG_CHAT_MODEL_NAME': 'Chatæ¨¡å‹åç§°', 
        'GFMRAG_CHAT_BASE_URL': 'ç¬¬ä¸‰æ–¹æœåŠ¡Base URL',
        'GFMRAG_CHAT_KEY': 'ChatæœåŠ¡APIå¯†é’¥'
    }
    
    env_status = {}
    print("\n1ï¸âƒ£ ç¯å¢ƒå˜é‡çŠ¶æ€:")
    for var, description in env_vars.items():
        value = os.getenv(var)
        if value:
            env_status[var] = value
            if var == 'GFMRAG_CHAT_KEY':
                print(f"   âœ… {var}: å·²è®¾ç½® (***éšè—***)")
            else:
                print(f"   âœ… {var}: {value}")
        else:
            env_status[var] = None
            print(f"   âŒ {var}: æœªè®¾ç½®")
    
    return env_status


def check_configuration_manager():
    """æ£€æŸ¥é…ç½®ç®¡ç†å™¨çš„çŠ¶æ€"""
    print("\n2ï¸âƒ£ é…ç½®ç®¡ç†å™¨çŠ¶æ€:")
    try:
        config_manager = get_config_manager()
        chat_config = config_manager.get_chat_config()
        
        print(f"   âœ… Provider: {chat_config.provider}")
        print(f"   âœ… Model: {chat_config.model_name}")
        print(f"   âœ… Base URL: {chat_config.base_url or 'é»˜è®¤'}")
        print(f"   âœ… API Key: {'å·²è®¾ç½®' if chat_config.api_key else 'æœªè®¾ç½®'}")
        
        return chat_config
    except Exception as e:
        print(f"   âŒ é…ç½®ç®¡ç†å™¨é”™è¯¯: {e}")
        return None


def test_hydra_config_loading():
    """æµ‹è¯•Hydraé…ç½®æ–‡ä»¶çš„åŠ è½½"""
    print("\n3ï¸âƒ£ Hydraé…ç½®æ–‡ä»¶æµ‹è¯•:")
    
    config_dir = project_root / "gfmrag" / "workflow" / "config"
    
    try:
        # æµ‹è¯• NER æ¨¡å‹é…ç½®
        with initialize_config_dir(config_dir=str(config_dir), version_base=None):
            cfg = compose(config_name="ner_model/llm_ner_model.yaml")
            print(f"   âœ… NERé…ç½®åŠ è½½æˆåŠŸ:")
            print(f"      - llm_api: {cfg.llm_api}")
            print(f"      - model_name: {cfg.model_name}")
            print(f"      - base_url: {cfg.base_url}")
            print(f"      - api_key: {'å·²è®¾ç½®' if cfg.api_key and cfg.api_key != 'null' else 'æœªè®¾ç½®'}")
        
        # æµ‹è¯• OpenIE æ¨¡å‹é…ç½®
        with initialize_config_dir(config_dir=str(config_dir), version_base=None):
            cfg = compose(config_name="openie_model/llm_openie_model.yaml")
            print(f"   âœ… OpenIEé…ç½®åŠ è½½æˆåŠŸ:")
            print(f"      - llm_api: {cfg.llm_api}")
            print(f"      - model_name: {cfg.model_name}")
            print(f"      - base_url: {cfg.base_url}")
            print(f"      - api_key: {'å·²è®¾ç½®' if cfg.api_key and cfg.api_key != 'null' else 'æœªè®¾ç½®'}")
            
        return True
    except Exception as e:
        print(f"   âŒ Hydraé…ç½®åŠ è½½å¤±è´¥: {e}")
        return False


def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–æ˜¯å¦æ­£å¸¸"""
    print("\n4ï¸âƒ£ æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•:")
    try:
        print("   ğŸ”„ æ­£åœ¨æµ‹è¯•LangChainæ¨¡å‹åˆå§‹åŒ–...")
        model = init_langchain_model()
        print(f"   âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {type(model).__name__}")
        
        # å°è¯•ç®€å•çš„æ¨¡å‹è°ƒç”¨ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            response = model.invoke("Hello, this is a test message.")
            print("   âœ… æ¨¡å‹è°ƒç”¨æµ‹è¯•æˆåŠŸ")
            print(f"      å“åº”é¢„è§ˆ: {response.content[:100]}...")
            return True
        except Exception as e:
            print(f"   âš ï¸ æ¨¡å‹è°ƒç”¨æµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œæˆ–è®¤è¯é—®é¢˜ï¼‰: {e}")
            return False
            
    except Exception as e:
        print(f"   âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def generate_configuration_recommendations(env_status: Dict[str, Any]):
    """ç”Ÿæˆé…ç½®å»ºè®®"""
    print("\n5ï¸âƒ£ é…ç½®å»ºè®®:")
    
    provider = env_status.get('GFMRAG_CHAT_PROVIDER')
    base_url = env_status.get('GFMRAG_CHAT_BASE_URL')
    api_key = env_status.get('GFMRAG_CHAT_KEY')
    
    if not provider:
        print("   ğŸ“ å»ºè®®è®¾ç½® GFMRAG_CHAT_PROVIDER ç¯å¢ƒå˜é‡")
        print("      ä¾‹å¦‚: export GFMRAG_CHAT_PROVIDER='third-party'")
    
    if provider == 'third-party' and not base_url:
        print("   ğŸ“ ä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡æ—¶ï¼Œå»ºè®®è®¾ç½® GFMRAG_CHAT_BASE_URL")
        print("      ä¾‹å¦‚: export GFMRAG_CHAT_BASE_URL='http://localhost:8000/v1'")
    
    if provider in ['openai', 'together', 'nvidia'] and not api_key:
        print(f"   ğŸ“ ä½¿ç”¨ {provider} æœåŠ¡æ—¶ï¼Œå»ºè®®è®¾ç½® GFMRAG_CHAT_KEY")
        print("      ä¾‹å¦‚: export GFMRAG_CHAT_KEY='your-api-key'")
    
    print("\n   ğŸ“‹ å®Œæ•´çš„ç¬¬ä¸‰æ–¹æœåŠ¡é…ç½®ç¤ºä¾‹:")
    print("      export GFMRAG_CHAT_PROVIDER='third-party'")
    print("      export GFMRAG_CHAT_MODEL_NAME='Qwen3-VL-30B-A3B-Instruct-FP8'")
    print("      export GFMRAG_CHAT_BASE_URL='http://192.168.110.11:8888/v1'")
    print("      export GFMRAG_CHAT_KEY='your-api-key'  # å¯é€‰")


def print_summary(results: Dict[str, bool]):
    """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)
    
    all_passed = all(results.values())
    
    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GFM-RAGé…ç½®æ­£å¸¸ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¸Šè¿°å»ºè®®å¹¶ä¿®å¤é…ç½®é—®é¢˜ã€‚")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    print("å¯åŠ¨ GFM-RAG ç¯å¢ƒå˜é‡é…ç½®éªŒè¯...")
    
    results = {}
    
    # 1. æ£€æŸ¥ç¯å¢ƒå˜é‡
    env_status = check_environment_variables()
    
    # 2. æ£€æŸ¥é…ç½®ç®¡ç†å™¨
    config_manager_ok = check_configuration_manager() is not None
    results["é…ç½®ç®¡ç†å™¨"] = config_manager_ok
    
    # 3. æµ‹è¯•Hydraé…ç½®åŠ è½½
    hydra_ok = test_hydra_config_loading()
    results["Hydraé…ç½®åŠ è½½"] = hydra_ok
    
    # 4. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
    model_ok = test_model_initialization()
    results["æ¨¡å‹åˆå§‹åŒ–"] = model_ok
    
    # 5. ç”Ÿæˆå»ºè®®
    generate_configuration_recommendations(env_status)
    
    # 6. æ‰“å°æ‘˜è¦
    print_summary(results)
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    return 0 if all(results.values()) else 1


if __name__ == "__main__":
    sys.exit(main())